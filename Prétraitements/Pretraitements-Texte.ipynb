{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prétraitements textuelles avec NLTK "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import des différentes bibliothèques:\n",
    "\n",
    "NLTK, unicodedata, contractions, inflect : Librairies orienté traitement de texte\n",
    "Scikit-Learn : Librairies orientés machine learning\n",
    "\n",
    "NLTK demande le téléchargement de certaints jeux de données pour pouvoir les utiliser, par exemple les stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/alcoz/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/alcoz/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/alcoz/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/alcoz/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package tagsets to /home/alcoz/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "import unicodedata\n",
    "import contractions\n",
    "import inflect\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import *\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('tagsets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import des datasets organisés en dossier avec le label comme nom du dossier.\n",
    "Fonction load_files (Scikit-Learn), l'encodage en UTF-8 est important, les fichiers sont importé en bytes.\n",
    "\n",
    "La fonction utilise une structure Bunch, les attributs interessant sont :\n",
    "   * data : Le tableau des différents texte avec une row est ègale à un fichier\n",
    "   * target : Le tableau des labels encodé en entier numérique\n",
    "   * target_names : Tableau des labels en String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Datasets_Train = load_files(\"../Datasets/Movies_Comment/train\", encoding = 'UTF-8')\n",
    "Datasets_Test = load_files(\"../Datasets/Movies_Comment/test\", encoding = 'UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My boyfriend and I went to watch The Guardian.At first I didn't want to watch it, but I loved the movie- It was definitely the best movie I have seen in sometime.They portrayed the USCG very well, it really showed me what they do and I think they should really be appreciated more.Not only did it teach but it was a really good movie. The movie shows what the really do and how hard the job is.I think being a USCG would be challenging and very scary. It was a great movie all around. I would suggest this movie for anyone to see.The ending broke my heart but I know why he did it. The storyline was great I give it 2 thumbs up. I cried it was very emotional, I would give it a 20 if I could!\n",
      "1\n",
      "pos\n"
     ]
    }
   ],
   "source": [
    "#Commentaire\n",
    "dataRaw = Datasets_Test.data\n",
    "print(Datasets_Test.data[0])\n",
    "\n",
    "#Label encodé\n",
    "print(Datasets_Test.target[0])\n",
    "\n",
    "#Label non encodé\n",
    "print(Datasets_Test.target_names[Datasets_Test.target[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prétraitements avec nltk\n",
    "\n",
    "Génération des StopWords et des GoWords avec extension des StopWords par des mots choisis par l'utilisateur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'movie', 'popcorn', 'the']\n"
     ]
    }
   ],
   "source": [
    "GoWords = ['not', 'nor', 'up', 'out', 'can']\n",
    "global OurStopWords\n",
    "OurStopWords = ['movie', 'popcorn','the']\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(OurStopWords)\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nltk est une bibliothèque de fonction utilisable pour le traitement de texte.\n",
    "### Pour pouvoir l'exploiter plus facilement, on va créer une fonction de pré-traitements et l'utiliser avec un vectorizer, un outils qui transforme les phrases en matrice de probabilité."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemmatisation et Lemmatisation\n",
    "La **Stemmatisation** (*ST*) et la **Lemmatisation** (*L*) sont deux fonctions qui ont pour objectif de réduire un mot pour en générer un version raccourci.\n",
    "\n",
    "La *ST* va réduire un mot en supprimant les préfixes et les suffixes, la sortie représente la racine du mot.\n",
    "\n",
    "La *L* consiste à ramener un terme à sa forme la plus simple (infinitif pour les verbes, Masculin/Singulier pour les noms, etc.)\n",
    "\n",
    "La *ST* ne vas pas forcément renvoyé un mot existant alors que la *L* oui\n",
    "\n",
    "Il existe plusieurs moteurs pour la *ST*, Ici on utilise le Snowball.\n",
    "Sa particularité est qu'il est crée pour plusieurs langues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Of Speech (*POS*)\n",
    "\n",
    "Liste des catégories de mots :\n",
    "* CC coordinating conjunction\n",
    "* CD cardinal digit\n",
    "* DT determiner\n",
    "* EX existential there (like: “there is” … think of it like “there exists”)\n",
    "* FW foreign word\n",
    "* IN preposition/subordinating conjunction\n",
    "* JJ adjective ‘big’\n",
    "* JJR adjective, comparative ‘bigger’\n",
    "* JJS adjective, superlative ‘biggest’\n",
    "* LS list marker 1)\n",
    "* MD modal could, will\n",
    "* NN noun, singular ‘desk’\n",
    "* NNS noun plural ‘desks’\n",
    "* NNP proper noun, singular ‘Harrison’\n",
    "* NNPS proper noun, plural ‘Americans’\n",
    "* PDT predeterminer ‘all the kids’\n",
    "* POS possessive ending parent’s\n",
    "* PRP personal pronoun I, he, she\n",
    "* PRP\\$ possessive pronoun my, his, hers\n",
    "* RB adverb very, silently,\n",
    "* RBR adverb, comparative better\n",
    "* RBS adverb, superlative best\n",
    "* RP particle give up\n",
    "* TO, to go ‘to’ the store.\n",
    "* UH interjection, errrrrrrrm\n",
    "* VB verb, base form take\n",
    "* VBD verb, past tense took\n",
    "* VBG verb, gerund/present participle taking\n",
    "* VBN verb, past participle taken\n",
    "* VBP verb, sing. present, non-3d take\n",
    "* VBZ verb, 3rd person sing. present takes\n",
    "* WDT wh-determiner which\n",
    "* WP wh-pronoun who, what\n",
    "* WP\\$ possessive wh-pronoun whose\n",
    "* WRB wh-abverb where, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(commentString):   \n",
    "    # Removing non ASCII characters\n",
    "    commentString = unicodedata.normalize('NFKD', commentString).encode(\"ascii\", \"ignore\").decode(\"utf-8\", 'ignore')\n",
    "\n",
    "    # Removing contractions\n",
    "    commentString = contractions.fix(commentString, slang = True)\n",
    "\n",
    "    # Tokenizing\n",
    "    tokenizedText = word_tokenize(commentString)\n",
    "\n",
    "    # Putting all words in lowercase\n",
    "    tokenizedText = [word.lower() for word in tokenizedText]\n",
    "\n",
    "    #Putting all words in uppercase\n",
    "    #tokenizedText = [word.upper() for word in tokenizedText]\n",
    "    \n",
    "    # Deleting ponctuations\n",
    "    tokenizedText = [word for word in tokenizedText if word.isalpha()]\n",
    "\n",
    "    # Removing stop words\n",
    "    tokenizedText = [word for word in tokenizedText if not word in stop_words]\n",
    "    \n",
    "    # Converting numbers\n",
    "    inflectEngine = inflect.engine()\n",
    "    newWords = []\n",
    "    for word in tokenizedText:\n",
    "        if word.isdigit():\n",
    "            newWords.append(inflectEngine.number_to_words(word))\n",
    "        else:\n",
    "            newWords.append(word)\n",
    "    tokenizedText = newWords\n",
    "\n",
    "    # Stemmatisation\n",
    "    stemmer = EnglishStemmer()\n",
    "    commentString = [stemmer.stem(word) for word in tokenizedText]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    commentString = [lemmatizer.lemmatize(word) for word in tokenizedText]\n",
    "\n",
    "    # Turning back tokens into a string\n",
    "    commentString = \"\".join([\" \" + i for i in tokenizedText]).strip()\n",
    "    \n",
    "    return commentString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('arabic', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish')\n"
     ]
    }
   ],
   "source": [
    "print(SnowballStemmer.languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My boyfriend and I went to watch The Guardian.At first I didn't want to watch it, but I loved the movie- It was definitely the best movie I have seen in sometime.They portrayed the USCG very well, it really showed me what they do and I think they should really be appreciated more.Not only did it teach but it was a really good movie. The movie shows what the really do and how hard the job is.I think being a USCG would be challenging and very scary. It was a great movie all around. I would suggest this movie for anyone to see.The ending broke my heart but I know why he did it. The storyline was great I give it 2 thumbs up. I cried it was very emotional, I would give it a 20 if I could!\n",
      "================\n",
      "boyfriend went watch first want watch loved definitely best seen portrayed uscg well really showed think really appreciated teach really good shows really hard job think uscg would challenging scary great around would suggest anyone ending broke heart know storyline great give thumbs cried emotional would give could\n",
      "[('boyfriend', 'VB'), ('went', 'VBD'), ('watch', 'NN'), ('first', 'RB'), ('want', 'JJ'), ('watch', 'NN'), ('loved', 'VBD'), ('definitely', 'RB'), ('best', 'JJS'), ('seen', 'VBN'), ('portrayed', 'VBN'), ('uscg', 'JJ'), ('well', 'RB'), ('really', 'RB'), ('showed', 'VBN'), ('think', 'VBP'), ('really', 'RB'), ('appreciated', 'VBN'), ('teach', 'NN'), ('really', 'RB'), ('good', 'JJ'), ('shows', 'NNS'), ('really', 'RB'), ('hard', 'JJ'), ('job', 'NN'), ('think', 'VBP'), ('uscg', 'NN'), ('would', 'MD'), ('challenging', 'VB'), ('scary', 'JJ'), ('great', 'JJ'), ('around', 'RP'), ('would', 'MD'), ('suggest', 'VB'), ('anyone', 'NN'), ('ending', 'VBG'), ('broke', 'VBD'), ('heart', 'NN'), ('know', 'VBP'), ('storyline', 'JJ'), ('great', 'JJ'), ('give', 'JJ'), ('thumbs', 'NNS'), ('cried', 'VBD'), ('emotional', 'JJ'), ('would', 'MD'), ('give', 'VB'), ('could', 'MD')]\n"
     ]
    }
   ],
   "source": [
    "print(dataRaw[0])\n",
    "clean = clean_text(dataRaw[0])\n",
    "print(\"================\")\n",
    "print(clean)\n",
    "print(nltk.pos_tag(nltk.word_tokenize(clean)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK Tag Helper : donne une explication pour chaque tag et des exemples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset('RB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application d'un vectorizer sur nos textes\n",
    "\n",
    "Les vectorizer permettent de calculer des occurences ou des calculs sur les occurences de mots ou de groupes de mots sur une liste de documents.\n",
    "Avec Scikit-Learn, on peut utiliser une fonction de prétraitement sur nos données avant d'appliquer la vectorisation.\n",
    "Les paramètres les plus importants sont:\n",
    "   * les N-grams: valeurs représentant le nombre de mots étudiés, si c'est un ensemble. La fonction prend un mots, ensuits le suivant, et ainsi de suite jusqu'au nombre choisis comme deuxième valeur.\n",
    "   * min_df et max-df: le vectoriseur va sélectionner que les valeurs appartenat à cette ensemble.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les 3 vectorizer présents sur Scikit-Learn:\n",
    "   * CountVectorizer : génère une matrice d'occurences des mots pour une liste de documents\n",
    "   * HashingVectorizer : Comme le count mais en utilisant une méthode de Hachage pour créer le tableau\n",
    "   * TfidfVectorizer : gènère une matrice TF-IDF pour la collection documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(preprocessor = clean_text, ngram_range = (1, 3), min_df = 0.01, max_df = 0.9)\n",
    "vectors = vectorizer.fit_transform(dataRaw)\n",
    "\n",
    "data = vectors.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Différentes bibliothèques utiles pour le NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pattern :** Une bibliothèque regroupant des outils de DataMining, de NLP, de Machine Learning et des modèles de graphe. <br>\n",
    "GitHUB : https://github.com/clips/pattern/wiki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spacy** : Une librarie au top de la recherche dans le NLT et orienté à l'implémentation de cas concret. La bibliothèque est composé de modèle statistiques pré-entrainé, des vecteurs de mots et des fonctions de tokenisation dans plus de 60 langues. <br>\n",
    "La bibliothèque est exploitable pour des conv networks, le parsage reconnaissance d'entité et des méthdoe de Deep Learning <br>\n",
    "GitHUB : https://github.com/explosion/spaCy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
